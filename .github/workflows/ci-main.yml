name: CI - Main Branch

on:
  push:
    branches:
      - main
  workflow_dispatch:

env:
  PYTHON_VERSION: '3.11'
  TERRAFORM_VERSION: '1.11.0'
  COMPOSE_PROJECT_NAME: streaming-etl-ci

jobs:
  lint-and-format:
    name: Lint and Format
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r src/producer/requirements.txt
        pip install -r src/streaming/requirements.txt
        pip install -r tests/requirements.txt

    - name: Set up Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: ${{ env.TERRAFORM_VERSION }}

    - name: Install terraform formatting tools
      run: |
        curl -s https://raw.githubusercontent.com/terraform-linters/tflint/master/install_linux.sh | bash

    - name: Python Code Formatting Check
      run: |
        black --check --diff src/ tests/ docker/

    - name: Python Linting and Auto-fixes
      run: |
        echo "Linting Python code with ruff..."
        ruff check --fix src/ tests/ docker/

    - name: Terraform Formatting Check
      run: |
        cd infra/terraform
        terraform fmt -check -recursive

    - name: Terraform Linting
      run: |
        echo "Linting Terraform code..."
        cd infra/terraform
        tflint --init
        tflint

    - name: Terraform Validation
      run: |
        cd infra/terraform
        terraform init -backend=false
        terraform validate

  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    needs: lint-and-format

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r tests/requirements.txt
        pip install -r src/producer/requirements.txt
        pip install -r src/streaming/requirements.txt

    - name: Run Unit Tests
      run: |
        cd tests
        python -m pytest \
          -v --tb=short --cov=../src --cov-report=xml --cov-report=html \
          --junitxml=test-results.xml

    - name: Upload Test Results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: test-results
        path: |
          tests/test-results.xml
          tests/htmlcov/
          tests/coverage.xml

    - name: Upload Coverage to Codecov
      uses: codecov/codecov-action@v3
      if: always()
      with:
        file: tests/coverage.xml
        flags: unittests
        name: codecov-umbrella

  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: unit-tests

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r tests/requirements.txt
        pip install -r src/producer/requirements.txt
        pip install -r src/streaming/requirements.txt

    - name: Create test environment file
      run: |
        cat > .env << EOF
        # Kafka Configuration
        KAFKA_BOOTSTRAP_SERVERS=localhost:9092
        KAFKA_TOPIC=iot-events-test

        # Producer Configuration
        PRODUCER_INTERVAL=1.0
        MAX_EVENTS=10

        # Spark Configuration
        CHECKPOINT_LOCATION=/tmp/spark-checkpoints-test
        OUTPUT_PATH=s3a://test-bucket-data/processed/

        # MinIO/S3 Configuration
        MINIO_ENDPOINT=http://localhost:9000
        AWS_ENDPOINT=http://localhost:9000
        AWS_ACCESS_KEY_ID=${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY=${{ secrets.AWS_SECRET_ACCESS_KEY }}
        MINIO_ROOT_USER=${{ secrets.MINIO_ROOT_USER }}
        MINIO_ROOT_PASSWORD=${{ secrets.MINIO_ROOT_PASSWORD }}

        # S3 Buckets
        AWS_S3_BUCKET_LANDING=test-bucket-landing
        AWS_S3_BUCKET_DATA=test-bucket-data

        # Historical Data Configuration
        HISTORICAL_DAYS=1

        # Grafana Configuration
        GRAFANA_ADMIN_PASSWORD=admin123
        EOF

    - name: Create required directories
      run: |
        mkdir -p tmp/spark-checkpoints
        mkdir -p config
        chmod 777 tmp/spark-checkpoints

    - name: Create minimal config files
      run: |
        # Create minimal prometheus config
        mkdir -p config
        cat > config/prometheus.yml << EOF
        global:
          scrape_interval: 15s
        scrape_configs:
          - job_name: 'prometheus'
            static_configs:
              - targets: ['localhost:9090']
        EOF

        # Create minimal JMX exporter config
        cat > config/jmx-exporter.yml << EOF
        rules:
          - pattern: ".*"
        EOF

    - name: Install Docker Compose
      run: |
        sudo curl -L "https://github.com/docker/compose/releases/download/v2.20.2/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
        sudo chmod +x /usr/local/bin/docker-compose
        docker-compose --version # Verify installation

    - name: Setup Infrastructure (equivalent to make setup-infrastructure)
      run: |
        echo "ðŸš€ Starting infrastructure services..."

        # Start core infrastructure services (equivalent to make start-infrastructure)
        docker-compose up -d zookeeper kafka minio prometheus grafana kafka-ui jmx-exporter

        echo "â³ Waiting for services to initialize..."
        sleep 30

    - name: Wait for services to be healthy
      run: |
        echo "ðŸ” Checking service health..."

        # Wait for Kafka to be ready
        echo "Waiting for Kafka..."
        timeout 120 bash -c 'until nc -z localhost 9092; do echo "Waiting for Kafka..."; sleep 5; done'

        # Wait for MinIO to be ready
        echo "Waiting for MinIO..."
        timeout 120 bash -c 'until curl -f http://localhost:9000/minio/health/live; do echo "Waiting for MinIO..."; sleep 5; done'

        # Additional wait for full initialization
        echo "Allowing additional time for service initialization..."
        sleep 20

        echo "âœ… All infrastructure services are ready!"

    - name: Verify Infrastructure Setup
      run: |
        echo "ðŸ” Verifying infrastructure setup..."

        # Check service status
        docker-compose ps

        # Test Kafka connectivity
        echo "Testing Kafka connectivity..."
        python3 -c "
        from kafka import KafkaAdminClient
        import sys
        try:
            admin = KafkaAdminClient(bootstrap_servers='localhost:9092', client_id='ci_test')
            topics = admin.list_topics()
            print(f'âœ… Kafka connection successful. Topics: {list(topics)}')
        except Exception as e:
            print(f'âŒ Kafka connection failed: {e}')
            sys.exit(1)
        "

    - name: Run S3 Tester (equivalent to docker-compose up s3-tester)
      run: |
        echo "ðŸ§ª Running S3 connectivity tests..."
        docker-compose build s3-tester
        docker-compose run --rm s3-tester

    - name: Start IoT Producer in background (equivalent to docker-compose up -d iot-producer)
      run: |
        echo "ðŸš€ Starting IoT Producer..."
        docker-compose up -d iot-producer

        echo "â³ Allowing producer to generate some data..."
        sleep 30

    - name: Run Kafka Consumer Tester (equivalent to docker-compose up kafka-consumer-tester)
      run: |
        echo "ðŸ§ª Running Kafka Consumer tests..."
        docker-compose build kafka-consumer-tester
        docker-compose run --rm kafka-consumer-tester

    - name: Run Integration Tests
      run: |
        echo "ðŸ§ª Running integration tests..."
        cd tests
        python -m pytest test_integration.py -v --tb=short --junitxml=integration-test-results.xml

    - name: Check Producer Logs
      if: always()
      run: |
        echo "ðŸ“‹ IoT Producer logs:"
        docker-compose logs iot-producer

    - name: Check Service Status
      if: always()
      run: |
        echo "ðŸ“Š Final service status:"
        docker-compose ps

    - name: Upload Integration Test Results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: integration-test-results
        path: |
          tests/integration-test-results.xml

    - name: Cleanup Infrastructure
      if: always()
      run: |
        echo "ðŸ§¹ Cleaning up infrastructure..."
        docker-compose down -v
        docker system prune -f

  docker-build:
    name: Docker Build Test
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests]

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Build Producer image
      uses: docker/build-push-action@v5
      with:
        context: .
        dockerfile: ./docker/producer/Dockerfile
        push: false
        tags: iot-producer:test
        cache-from: type=gha
        cache-to: type=gha,mode=max

    - name: Build Streaming image
      uses: docker/build-push-action@v5
      with:
        context: .
        dockerfile: ./docker/streaming/Dockerfile
        push: false
        tags: spark-streaming:test
        cache-from: type=gha
        cache-to: type=gha,mode=max

    - name: Build S3 Tester image
      uses: docker/build-push-action@v5
      with:
        context: ./tests/s3_tester
        push: false
        tags: s3-tester:test
        cache-from: type=gha
        cache-to: type=gha,mode=max

    - name: Build Kafka Consumer Tester image
      uses: docker/build-push-action@v5
      with:
        context: ./tests/kafka_consumer_tester
        push: false
        tags: kafka-consumer-tester:test
        cache-from: type=gha
        cache-to: type=gha,mode=max

    - name: Test Docker Images
      run: |
        echo "ðŸ§ª Testing built Docker images..."

        # Test producer image
        docker run --rm iot-producer:test python3 -c "import producer; print('âœ… Producer image OK')"

        # Test s3-tester image
        docker run --rm s3-tester:test python3 -c "import test_s3; print('âœ… S3 tester image OK')"

        # Test kafka-consumer-tester image
        docker run --rm kafka-consumer-tester:test python3 -c "import test_kafka_consumer; print('âœ… Kafka consumer tester image OK')"

  end-to-end-test:
    name: End-to-End Docker Test
    runs-on: ubuntu-latest
    needs: docker-build

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Create test environment
      run: |
        # Create directories
        mkdir -p tmp/spark-checkpoints
        mkdir -p config
        chmod 777 tmp/spark-checkpoints

        # Create minimal config files
        cat > config/prometheus.yml << EOF
        global:
          scrape_interval: 15s
        scrape_configs:
          - job_name: 'prometheus'
            static_configs:
              - targets: ['localhost:9090']
        EOF

        cat > config/jmx-exporter.yml << EOF
        rules:
          - pattern: ".*"
        EOF

        # Create test .env
        cat > .env << EOF
        KAFKA_BOOTSTRAP_SERVERS=localhost:9092
        KAFKA_TOPIC=iot-events-e2e
        PRODUCER_INTERVAL=1.0
        MAX_EVENTS=5
        MINIO_ENDPOINT=http://localhost:9000
        AWS_ENDPOINT=http://localhost:9000
        AWS_ACCESS_KEY_ID=minioadmin
        AWS_SECRET_ACCESS_KEY=minioadmin
        MINIO_ROOT_USER=minioadmin
        MINIO_ROOT_PASSWORD=minioadmin
        AWS_S3_BUCKET_LANDING=e2e-bucket-landing
        AWS_S3_BUCKET_DATA=e2e-bucket-data
        GRAFANA_ADMIN_PASSWORD=admin123
        EOF

    - name: Run End-to-End Docker Compose Test
      run: |
        echo "ðŸš€ Running end-to-end Docker Compose test..."

        # Start infrastructure services (equivalent to make setup-infrastructure)
        echo "Starting infrastructure services..."
        docker-compose up -d zookeeper kafka minio

        # Wait for services
        echo "Waiting for services to be ready..."
        sleep 60

        # Check service health
        echo "Checking service status..."
        docker-compose ps

        # Run S3 test (equivalent to docker-compose up s3-tester)
        echo "Running S3 connectivity test..."
        docker-compose run --rm s3-tester

        # Start producer briefly (equivalent to docker-compose up -d iot-producer)
        echo "Starting IoT producer..."
        docker-compose up -d iot-producer
        sleep 30

        # Run kafka consumer test (equivalent to docker-compose up kafka-consumer-tester)
        echo "Running Kafka consumer test..."
        docker-compose run --rm kafka-consumer-tester

        # Check logs
        echo "Checking producer logs..."
        docker-compose logs iot-producer

        echo "âœ… End-to-end test completed successfully!"

    - name: Cleanup
      if: always()
      run: |
        echo "ðŸ§¹ Cleaning up end-to-end test environment..."
        docker-compose down -v
        docker system prune -f

  notify-success:
    name: Notify Success
    runs-on: ubuntu-latest
    needs: [lint-and-format, unit-tests, integration-tests, docker-build, end-to-end-test]
    if: success()

    steps:
    - name: Success Notification
      run: |
        echo "ðŸŽ‰ All CI checks passed successfully!"
        echo "âœ… Lint and format checks passed"
        echo "âœ… Unit tests passed"
        echo "âœ… Integration tests passed (with infrastructure setup)"
        echo "âœ… Docker images built successfully"
        echo "âœ… End-to-end tests passed"
        echo "ðŸš€ Ready for deployment"

  notify-failure:
    name: Notify Failure
    runs-on: ubuntu-latest
    needs: [lint-and-format, unit-tests, integration-tests, docker-build, end-to-end-test]
    if: failure()

    steps:
    - name: Failure Notification
      run: |
        echo "âŒ CI pipeline failed!"
        echo "Please check the failed jobs and fix the issues."
        echo "Lint and Format: ${{ needs.lint-and-format.result }}"
        echo "Unit Tests: ${{ needs.unit-tests.result }}"
        echo "Integration Tests: ${{ needs.integration-tests.result }}"
        echo "Docker Build: ${{ needs.docker-build.result }}"
        echo "End-to-End Test: ${{ needs.end-to-end-test.result }}"
        exit 1
