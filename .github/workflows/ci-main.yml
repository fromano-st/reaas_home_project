name: CI - Main Branch

on:
  push:
    branches:
      - main
  workflow_dispatch:

env:
  PYTHON_VERSION: '3.11'
  TERRAFORM_VERSION: '1.11.0'
  COMPOSE_PROJECT_NAME: streaming-etl-ci

jobs:
  lint-and-format:
    name: Lint and Format
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r src/producer/requirements.txt
        pip install -r src/streaming/requirements.txt
        pip install -r tests/requirements.txt

    - name: Set up Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: ${{ env.TERRAFORM_VERSION }}

    - name: Install terraform formatting tools
      run: |
        curl -s https://raw.githubusercontent.com/terraform-linters/tflint/master/install_linux.sh | bash

    - name: Python Code Formatting Check
      run: |
        black --check --diff src/ tests/ docker/

    - name: Python Linting and Auto-fixes
      run: |
        echo "Linting Python code with ruff..."
        ruff check --fix src/ tests/ docker/

    - name: Terraform Formatting Check
      run: |
        cd infra/terraform
        terraform fmt -check -recursive

    - name: Terraform Linting
      run: |
        echo "Linting Terraform code..."
        cd infra/terraform
        tflint --init
        tflint

    - name: Terraform Validation
      run: |
        cd infra/terraform
        terraform init -backend=false
        terraform validate

  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    needs: lint-and-format

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r tests/requirements.txt
        pip install -r src/producer/requirements.txt
        pip install -r src/streaming/requirements.txt

    - name: Run Unit Tests
      run: |
        cd tests
        python -m pytest \
          -v --tb=short --cov=../src --cov-report=xml --cov-report=html \
          --junitxml=test-results.xml

    - name: Upload Test Results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: test-results
        path: |
          tests/test-results.xml
          tests/htmlcov/
          tests/coverage.xml

    - name: Upload Coverage to Codecov
      uses: codecov/codecov-action@v3
      if: always()
      with:
        file: tests/coverage.xml
        flags: unittests
        name: codecov-umbrella

  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: unit-tests

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r tests/requirements.txt
        pip install -r src/producer/requirements.txt
        pip install -r src/streaming/requirements.txt

    - name: Create test environment file
      run: |
        cat > .env << EOF
        # Kafka Configuration
        KAFKA_BOOTSTRAP_SERVERS=kafka:29092
        KAFKA_TOPIC=iot-events-test

        # Producer Configuration
        PRODUCER_INTERVAL=1.0
        MAX_EVENTS=10

        # Spark Configuration
        CHECKPOINT_LOCATION=/tmp/spark-checkpoints-test
        OUTPUT_PATH=s3a://test-bucket-data/processed/

        # MinIO/S3 Configuration
        MINIO_ENDPOINT=http://localhost:9000
        AWS_ENDPOINT=http://minio:9000
        AWS_ACCESS_KEY_ID=${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY=${{ secrets.AWS_SECRET_ACCESS_KEY }}
        MINIO_ROOT_USER=${{ secrets.MINIO_ROOT_USER }}
        MINIO_ROOT_PASSWORD=${{ secrets.MINIO_ROOT_PASSWORD }}

        # S3 Buckets
        AWS_S3_BUCKET_LANDING=test-bucket-landing
        AWS_S3_BUCKET_DATA=test-bucket-data

        # Historical Data Configuration
        HISTORICAL_DAYS=1

        # Grafana Configuration
        GRAFANA_ADMIN_PASSWORD=admin123
        EOF

    - name: Create required directories
      run: |
        mkdir -p tmp/spark-checkpoints
        mkdir -p config
        chmod 777 tmp/spark-checkpoints

    - name: Create minimal config files
      run: |
        # Create minimal prometheus config
        mkdir -p config
        cat > config/prometheus.yml << EOF
        global:
          scrape_interval: 15s
        scrape_configs:
          - job_name: 'prometheus'
            static_configs:
              - targets: ['localhost:9090']
        EOF

        # Create minimal JMX exporter config
        cat > config/jmx-exporter.yml << EOF
        rules:
          - pattern: ".*"
        EOF

    - name: Install Docker Compose
      run: |
        sudo curl -L "https://github.com/docker/compose/releases/download/v2.20.2/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
        sudo chmod +x /usr/local/bin/docker-compose
        docker-compose --version # Verify installation

    - name: Setup Infrastructure (equivalent to make setup-infrastructure)
      run: |
        echo "🚀 Starting infrastructure services..."

        # Start core infrastructure services (equivalent to make start-infrastructure)
        docker-compose up -d zookeeper kafka minio prometheus grafana kafka-ui jmx-exporter

        echo "⏳ Waiting for services to initialize..."
        sleep 30

    - name: Wait for services to be healthy
      run: |
        echo "🔍 Checking service health..."

        # Wait for Kafka to be ready
        echo "Waiting for Kafka..."
        timeout 120 bash -c 'until nc -z localhost 9092; do echo "Waiting for Kafka..."; sleep 5; done'

        # Wait for MinIO to be ready
        echo "Waiting for MinIO..."
        timeout 120 bash -c 'until curl -f http://localhost:9000/minio/health/live; do echo "Waiting for MinIO..."; sleep 5; done'

        # Additional wait for full initialization
        echo "Allowing additional time for service initialization..."
        sleep 20

        echo "✅ All infrastructure services are ready!"

    - name: Verify Infrastructure Setup
      run: |
        echo "🔍 Verifying infrastructure setup..."

        # Check service status
        docker-compose ps

        # Test Kafka connectivity
        echo "Testing Kafka connectivity..."
        python3 -c "
        from kafka import KafkaAdminClient
        import sys
        try:
            admin = KafkaAdminClient(bootstrap_servers='localhost:9092', client_id='ci_test')
            topics = admin.list_topics()
            print(f'✅ Kafka connection successful. Topics: {list(topics)}')
        except Exception as e:
            print(f'❌ Kafka connection failed: {e}')
            sys.exit(1)
        "

    - name: Run S3 Tester (equivalent to docker-compose up s3-tester)
      run: |
        echo "🧪 Running S3 connectivity tests..."
        docker-compose build s3-tester
        docker-compose run --rm s3-tester

    - name: Start IoT Producer in background (equivalent to docker-compose up -d iot-producer)
      run: |
        echo "🚀 Starting IoT Producer..."
        docker-compose up -d iot-producer

        echo "⏳ Allowing producer to generate some data..."
        sleep 30

    - name: Run Kafka Consumer Tester (equivalent to docker-compose up kafka-consumer-tester)
      run: |
        echo "🧪 Running Kafka Consumer tests..."
        docker-compose build kafka-consumer-tester
        docker-compose run --rm kafka-consumer-tester

    - name: Run Integration Tests
      run: |
        echo "🧪 Running integration tests..."
        cd tests
        python -m pytest test_integration.py -v --tb=short --junitxml=integration-test-results.xml

    - name: Check Producer Logs
      if: always()
      run: |
        echo "📋 IoT Producer logs:"
        docker-compose logs iot-producer

    - name: Check Service Status
      if: always()
      run: |
        echo "📊 Final service status:"
        docker-compose ps

    - name: Upload Integration Test Results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: integration-test-results
        path: |
          tests/integration-test-results.xml

    - name: Cleanup Infrastructure
      if: always()
      run: |
        echo "🧹 Cleaning up infrastructure..."
        docker-compose down -v
        docker system prune -f

  notify-success:
    name: Notify Success
    runs-on: ubuntu-latest
    needs: [lint-and-format, unit-tests, integration-tests]
    if: success()

    steps:
    - name: Success Notification
      run: |
        echo "🎉 All CI checks passed successfully!"
        echo "✅ Lint and format checks passed"
        echo "✅ Unit tests passed"
        echo "✅ Integration tests passed (with infrastructure setup)"
        echo "🚀 Ready for deployment"

  notify-failure:
    name: Notify Failure
    runs-on: ubuntu-latest
    needs: [lint-and-format, unit-tests, integration-tests]
    if: failure()

    steps:
    - name: Failure Notification
      run: |
        echo "❌ CI pipeline failed!"
        echo "Please check the failed jobs and fix the issues."
        echo "Lint and Format: ${{ needs.lint-and-format.result }}"
        echo "Unit Tests: ${{ needs.unit-tests.result }}"
        echo "Integration Tests: ${{ needs.integration-tests.result }}"
        exit 1
